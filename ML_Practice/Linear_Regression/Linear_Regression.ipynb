{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf0L-kDl3bNp"
      },
      "source": [
        "#Generalized simple/multiple linear regressor\n",
        "This script applies basic ML techniques such as OLS gradient descent to produce a simple/multiple linear regression model to make predictions based on previously unseen data.\n",
        "\n",
        "My intention was to write this script in such a way that it could, with limited modification, be applied to any simple/multiple linear regression problem/dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz-qhdrG4CF2"
      },
      "source": [
        "# Initial steps\n",
        "The first steps I took in writing this script was to import a handful of useful modules for importing and manipulating data.\n",
        "\n",
        "With these imports complete, I set the constant learning rate and number of epochs. I then imported both the training and test data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt1UoXg2tKli"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_EPOCHS = 1000\n",
        "train = pd.read_csv(\"./train_data.csv\")\n",
        "test = pd.read_csv(\"./test_data.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbesHvDh4Z6l"
      },
      "source": [
        "# Gradient descent algorithm\n",
        "The following two functions implement the weight and bias update schema that serves as the heart of the \"learning\" that takes place.\n",
        "\n",
        "updateWeights serves as a high-level function for fetching the weight updates and making the appropriate arithmetic updates.\n",
        "\n",
        "calculateWeightUpdates actually calculates, via gradient descent and treating the entire training dataset as the batch size, the appropriate weight and bias updates. The gradient calculation was done assuming an OLS loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h95b0Dp3Wjk"
      },
      "source": [
        "def updateWeights(weights, b, predictedY, trueY, trainX, learningRate):\n",
        "  BUpdate, weightUpdates = calculateWeightUpdates(weights,b,predictedY,trueY,trainX)\n",
        "  b = b - learningRate * BUpdate\n",
        "  weights = weights -learningRate * weightUpdates\n",
        "  return b, weights\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUUm9iTsyg4W"
      },
      "source": [
        "def calculateWeightUpdates(weights, b, predictedY, trueY, trainX):\n",
        "  BUpdate = 0\n",
        "  weightUpdates = np.zeros(shape=weights.shape)\n",
        "  for exampleIndex in range(0,len(predictedY)):\n",
        "    BUpdate += -2*(trueY[exampleIndex] - predictedY[exampleIndex])\n",
        "    for weightIndex in range(0, len(weightUpdates)):\n",
        "      weightUpdates[weightIndex] += -2*trainX[exampleIndex][weightIndex]*(trueY[exampleIndex] - predictedY[exampleIndex])\n",
        "  BUpdate = BUpdate / len(predictedY)\n",
        "  for weight in weightUpdates:\n",
        "    weight = weight/len(predictedY)\n",
        "  return BUpdate, weightUpdates\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhhHGkQ5Ppb"
      },
      "source": [
        "# Evaluation\n",
        "The following function implements a fairly basic method of evaluation by computing the MSE for the given set (training or test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc4qG5Fv-uYY"
      },
      "source": [
        "def calculateMSE(predictedY, trueY):\n",
        "  N = len(trueY)\n",
        "  sumSquares = 0\n",
        "  for example in range(0, len(predictedY)):\n",
        "    sumSquares += (trueY[example] - predictedY[example])**2\n",
        "  return sumSquares/N\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OCey_WZvvb6"
      },
      "source": [
        "# Data manipulations and training loop\n",
        "The next bit of code creates separate data frames for the dependent and independent variables. The independent variables are normalized to avoid any issues with scaling. This can be adjusted as needed given the dataset.\n",
        "\n",
        "The last section of code forms the training loop which computes the predictions based on the current weights via matrix multiplication, passes these predictions to the graident descent algorithm, and repeats for a specified number of epochs. The MSE is then calculated for both the training and test data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arpuodJ0u1nS",
        "outputId": "579e37e6-799e-4bb0-d52d-4678f6fed679"
      },
      "source": [
        "trainY = train[\"octanenum\"].to_numpy()\n",
        "trainY = trainY.reshape(len(trainY),1)\n",
        "trainX = train[[\"material1\", \"material2\", \"material3\", \"condition\"]]\n",
        "trainX = (trainX - trainX.min())/(trainX.max() - trainX.min())\n",
        "trainX = trainX.to_numpy()\n",
        "\n",
        "testY = test[\"octanenum\"].to_numpy()\n",
        "testY = testY.reshape(len(testY),1)\n",
        "testX = test[[\"material1\", \"material2\", \"material3\", \"condition\"]]\n",
        "testX = (testX - testX.min())/(testX.max() - testX.min())\n",
        "testX = testX.to_numpy()\n",
        "\n",
        "weights = np.random.rand(len(trainX[0]),1)\n",
        "b = 0\n",
        "\n",
        "for i in range(0, NUM_EPOCHS):\n",
        "  predictedY = np.matmul(trainX,weights) + b\n",
        "  b, weights = updateWeights(weights, b, predictedY, trainY, trainX, LEARNING_RATE)\n",
        "predictedTrainY = np.matmul(trainX, weights) + b\n",
        "print(\"Training MSE: \" + str(calculateMSE(predictedTrainY, trainY)[0]))\n",
        "predictedTestY = np.matmul(testX, weights) + b\n",
        "print(\"Test MSE: \" + str(calculateMSE(predictedTestY, testY)[0]))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MSE: 74.66440813562025\n",
            "Test MSE: 174.00120226760654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFI7ZAL3yfAd"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvkjkKNxv7TT"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}