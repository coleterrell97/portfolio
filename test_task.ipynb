{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coleterrell97/portfolio/blob/master/test_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUzMhLlvWT-Z"
      },
      "source": [
        "# Importing and Cleaning Data\r\n",
        "The first step in the process is to import the data such that it can be manipulated with Python. This is done using Pandas.\r\n",
        "\r\n",
        "From here, the next objective is to clean/simplify the data as much as possible. This will help limit inaccuracies in the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEaZ9Xj5Vfo2"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from scipy import stats\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "\r\n",
        "combined_data = pd.read_csv(\"./drive/MyDrive/test_task/test_task.csv\")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmrTStiT8F3K",
        "outputId": "5928e8f6-e0e1-4e48-fc1b-35513074f256"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrvAVa-GWre9"
      },
      "source": [
        "# Data Cleaning Process\r\n",
        "The data cleaning process for this data set will include multiple steps. Each of these steps will be described in detail (as well as the rationale behind the step) prior to the execution of the code for that step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyfiyrG0W8SQ"
      },
      "source": [
        "## Step 1: Removing the 'Total Players' Column\r\n",
        "This feature is superfluous, as it can be calculated easily from the Team 1 and Team 2 features. Removing this column simply makes the data easier to digest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaOKvaebXLbC"
      },
      "source": [
        "combined_data.drop(columns=[\"Total Players\"], inplace=True)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1jbpTjtXvO5"
      },
      "source": [
        "## Step 2: Converting the Number of Goalkeepers Column to a Binary Feature\r\n",
        "There are no instances where one team was allowed a keeper and the other team was not. Teams were either both allowed keepers or neither were. In this case, the data can be simplified by converting all of the 2's in this column to 1's, making it a proper binary feature signifying if keepers were allowed or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJIiy3tWYNdY"
      },
      "source": [
        "combined_data[\"Goalkeeper\"].replace(2,1, inplace=True)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIJVJaVZceW"
      },
      "source": [
        "## Step 3: Remove \"Not Specified\" Column\r\n",
        "It is unclear what this column's purpose is. The vast majority of the values present in this column are 0, so deleting this feature entirely should have little impact on the model's behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gx8Uno2ZzxS"
      },
      "source": [
        "combined_data.drop(columns=[\"Not specified\"], inplace=True)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmysYcRZ34Y"
      },
      "source": [
        "## Step 4: Convert Active and Passive Recovery Features to Binary Features\r\n",
        "The minute values stored in these columns can easily be back-calculated by looking at the total number of rest minutes. The omitted feature value in this case would be that the type of recovery is unspecified (when both passive and active features are 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PJG-5vJanAl"
      },
      "source": [
        "for index, row in combined_data.iterrows():\r\n",
        "    if row[\"P.R.\"] > 0:\r\n",
        "      combined_data.at[index, \"P.R.\"] = 1\r\n",
        "    if row[\"A.R.\"] > 0:\r\n",
        "      combined_data.at[index, \"A.R.\"] = 1"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riyxKEgYb2Mt"
      },
      "source": [
        "## Step 5: Add Aspect Ratio Column\r\n",
        "Based on the information presented in the provided Master's thesis, it seems that the aspect ratio of the pitch is a valuable piece of information that was not directly included in the data set. This is an easy calculation, and it will allow for the removal of superfluous features of length, width, and area of the pitch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUwFRvTHcLbF"
      },
      "source": [
        "aspect_ratio = combined_data[\"Pitch Length\"]/combined_data[\"Pitch Width\"]\r\n",
        "combined_data.insert(9, \"Aspect Ratio\", aspect_ratio)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lY7cGdjeyi1"
      },
      "source": [
        "## Step 6: Removing Last Unneccessary Columns\r\n",
        "I decided to remove the columns for pitch width, length, and area, because they can all be calculated in reverse using the remaining features.\r\n",
        "\r\n",
        "Additionally, I chose to remove the load/rest ratio column, as this is derived from the load and rest columns that already exist. It provides no additional data to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxBBASk6e9ju"
      },
      "source": [
        "combined_data.drop(columns=[\"Pitch Length\", \"Pitch Width\", \"Pitch Area\", \"Load/Rest Ratio\"], inplace=True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm_uoTY7CI_4"
      },
      "source": [
        "## Step 7: Removing Rows with no Labels\n",
        "These columns are of little use to training or testing the model, and filling in these missing values would be almost entirely guesswork."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPb5abIACfGh"
      },
      "source": [
        "for index, row in combined_data.iterrows():\n",
        "  if row[\"%HRmax\"] == 0 and row[\"%HRres\"] == 0 and row[\"Blood lactate\"] == 0 and row[\"RPE (CR10)\"] == 0:\n",
        "    combined_data.drop(index, axis = 0, inplace=True)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iBAtLWQz5Ui"
      },
      "source": [
        "## Step 8: Data Normalization\r\n",
        "To prevent any bias due to feature scaling, I normalized all the data such that all features have a range between 0,1 inclusive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pe4lgvL0DTd"
      },
      "source": [
        "combined_data.iloc[:, 0:10]=(combined_data.iloc[:, 0:10]-combined_data.iloc[:, 0:10].min())/(combined_data.iloc[:, 0:10].max()-combined_data.iloc[:, 0:10].min())"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGGWZiXniDqG"
      },
      "source": [
        "# Train vs. Test Sets\r\n",
        "The next important step in the process is to divide the combined data set into test and train sets. I will then take roughly 80% of the records for the training set and roughly 20% of the records for the test set. The rows were shuffled prior to input into the model (in Excel) to help eliminate any bias present in their ordering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4oRfLZqiff5"
      },
      "source": [
        "train_set = combined_data.copy().iloc[:396, :]\r\n",
        "test_set = combined_data.copy().iloc[396:, :]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtBN5xU9frxL"
      },
      "source": [
        "# Filling in Missing Data\r\n",
        "The next part of the process is to fill in missing label data. This can be done in multiple ways. The first method I intend to try is to assume a linear relationship between each of the indicators of exercise load. Seeing as how they are all supposedly measuring the same thing (at a high level) they should rise and fall in value together. It is possible to validate that these features have a linear relationship by calculating the correlation matrix for these columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p2BB4X3gJEv",
        "outputId": "339c9d13-f9b9-4c03-e4d4-c34612d40f33"
      },
      "source": [
        "labels_train = train_set[[\"%HRmax\", \"%HRres\", \"Blood lactate\", \"RPE (CR10)\"]].copy()\r\n",
        "for index, row in labels_train.iterrows():\r\n",
        "    if row[\"%HRmax\"] == 0 or row[\"%HRres\"] == 0 or row[\"Blood lactate\"] == 0 or row[\"RPE (CR10)\"] == 0:\r\n",
        "      labels_train.drop(index, axis=0, inplace=True)\r\n",
        "\r\n",
        "labels_correlation = labels_train.corr()\r\n",
        "print(labels_correlation)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 %HRmax    %HRres  Blood lactate  RPE (CR10)\n",
            "%HRmax         1.000000  0.992338       0.600295    0.191919\n",
            "%HRres         0.992338  1.000000       0.502976    0.279248\n",
            "Blood lactate  0.600295  0.502976       1.000000   -0.399668\n",
            "RPE (CR10)     0.191919  0.279248      -0.399668    1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EohokEz6jnJL"
      },
      "source": [
        "From this exercise, one can see that %HRmax and %HRres have are almost perfectly collinear. Thus, we can find the line of best fit between these featuers and fill in missing data accordingly.\n",
        "\n",
        "The relationship between %HRmax and blood lactate is less strictly linear, but this method should still serve well enough for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD5114E-IR-5"
      },
      "source": [
        "# Linear Regression for %HRmax, %HRres, and Blood lactate Categories\n",
        "To fill in the missing data for the HRmax, HRres, and Blood lactate categories, I intend to calculate the best fit line between these variables. These linear models will enable me to fill in the missing data in a way that is more precise than simply using the median or mean values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGT8VIH9JDwc"
      },
      "source": [
        "#calculating the lines of best fit\n",
        "lin_regression_HRmax_independent = stats.linregress(labels_train[\"%HRmax\"], labels_train[\"%HRres\"])\n",
        "lin_regression_HRres_independent = stats.linregress(labels_train[\"%HRres\"], labels_train[\"%HRmax\"])\n",
        "lin_regression_HRmax_independent_blood_lactate = stats.linregress(labels_train[\"%HRmax\"], labels_train[\"Blood lactate\"])\n",
        "lin_regression_blood_lactate_independent_max = stats.linregress(labels_train[\"Blood lactate\"], labels_train[\"%HRmax\"])\n",
        "\n",
        "#filling in missing data using the line of best fit\n",
        "for index, row in train_set.iterrows():\n",
        "    if (row[\"%HRres\"] == 0 and row[\"%HRmax\"] != 0):\n",
        "      train_set.at[index, \"%HRres\"] = lin_regression_HRmax_independent.intercept + lin_regression_HRmax_independent.slope * row[\"%HRmax\"]\n",
        "    if (row[\"%HRmax\"] == 0 and row[\"%HRres\"] != 0):\n",
        "      train_set.at[index, \"%HRmax\"] = lin_regression_HRres_independent.intercept + lin_regression_HRres_independent.slope * row[\"%HRres\"]\n",
        "    if (row[\"Blood lactate\"] == 0 and row[\"%HRmax\"] != 0):\n",
        "      train_set.at[index, \"Blood lactate\"] = lin_regression_HRmax_independent_blood_lactate.intercept + lin_regression_HRmax_independent_blood_lactate.slope * row[\"%HRmax\"]\n",
        "    if (row[\"Blood lactate\"] != 0 and row[\"%HRmax\"] == 0):\n",
        "      train_set.at[index, \"%HRmax\"] = lin_regression_blood_lactate_independent_max.intercept + lin_regression_blood_lactate_independent_max.slope * row[\"Blood lactate\"]\n",
        "    #I threw the first if statement in again at the end to make up for the fact that new values may have been added to the HRmax feature\n",
        "    if (row[\"%HRres\"] == 0 and row[\"%HRmax\"] != 0):\n",
        "      train_set.at[index, \"%HRres\"] = lin_regression_HRmax_independent.intercept + lin_regression_HRmax_independent.slope * row[\"%HRmax\"]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdtMcRkRLRb6"
      },
      "source": [
        "# Filling Data for RPE (CR10)\n",
        "Because there is little to no linear relationship between RPE (CR10) and any of the other labels, I will use the mean value of this column for all missing values in the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqF06mtzLoDn"
      },
      "source": [
        "#calculates the mean (does not include rows with missing values)\n",
        "mean = train_set[train_set[\"RPE (CR10)\"] != 0][\"RPE (CR10)\"].mean()\n",
        "train_set[\"RPE (CR10)\"].replace(0, mean, inplace = True)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8IZIXRrP-Zn"
      },
      "source": [
        "Any record that is still missing data in the label columns are dropped entirely from the training set. This is reasonable, as there are only a handful of data points that are still missing label data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWqoGku9QGf0"
      },
      "source": [
        "for index, row in train_set.iterrows():\n",
        "  if row[\"%HRmax\"] == 0 or row[\"%HRres\"] == 0 or row[\"Blood lactate\"] == 0 or row[\"RPE (CR10)\"] == 0:\n",
        "    train_set.drop(index, axis = 0, inplace=True)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAiWz6ByH6Iw"
      },
      "source": [
        "# Defining and Training the Model\n",
        "In this step, I used the Tensorflow.Keras package to define a neural network that features two hidden layers and uses relu activation. I chose mean absolute percentage error for the loss function, as the labels are of varying scales. Using the absolute percent error should help to weight loss on each of the labels equally and ignore their differences in scale.\n",
        "\n",
        "###Cross-Validation\n",
        "To verify the functionality of the model without contaminating the test data that has been set aside, I used 4-fold cross-validation. This process allows me to tune the architecture and any hyperparameters at will without worrying about fitting to the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCYMurY1pxH"
      },
      "source": [
        "#The data frame contained some NA rows. Unsure why this is, but these rows can simply be dropped.\r\n",
        "train_set.dropna(inplace=True)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(500, input_dim = 10, activation=\"relu\"))\r\n",
        "model.add(Dense(250, activation=\"relu\"))\r\n",
        "model.add(Dense(4, activation=\"linear\"))\r\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=\"adam\")\r\n",
        "model.save_weights(\"initial\")\r\n",
        "\r\n",
        "KFold_loss = []\r\n",
        "weights = []\r\n",
        "split = 0\r\n",
        "kf = KFold(n_splits = 4)\r\n",
        "kf.get_n_splits(train_set)\r\n",
        "for train_index, test_index in kf.split(train_set):\r\n",
        "  model.load_weights(\"initial\")\r\n",
        "  X_train, X_test = train_set.iloc[train_index, :10], train_set.iloc[test_index, :10]\r\n",
        "  y_train, y_test = train_set.iloc[train_index, 10:], train_set.iloc[test_index, 10:]\r\n",
        "  model.fit(X_train, y_train, epochs=400, batch_size=32)\r\n",
        "  KFold_loss.append(model.evaluate(X_test, y_test))\r\n",
        "  model.save_weights(str(split))\r\n",
        "  weights.append(str(split))\r\n",
        "  split += 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU5AMA-8GQmj"
      },
      "source": [
        "It should be noted that the architecture used in this model was selected based on the assumption that the data would not be linearly seperable. Thus, a neural net with at least one hidden layer was necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdAahQna_DzC",
        "outputId": "6a04a733-ab77-4fb3-c66f-e42c20b866ac"
      },
      "source": [
        "#load the weights of the model that performed best during cross-validation\r\n",
        "best_weights = KFold_loss.index(min(KFold_loss))\r\n",
        "model.load_weights(weights[best_weights])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f77a115bb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzDykano_tSD"
      },
      "source": [
        "# Testing the Model\r\n",
        "The final step of this process is to test the model on the training data that was set aside. The metric I used to quantify the model's performance on the test data is the mean absolute percent error. This is calculated by finding the absolute percent error for each label within each example, taking the average of these values, and then averaging these averages over the entire test data set.\r\n",
        "\r\n",
        "$(\\frac{1}{examples}\\sum\\limits_{j=1}^{j=examples}(\\frac{1}{labels}\\sum\\limits_{n=1}^{n=labels}|\\frac{Y_{predicted} - Y_{actual}}{Y_{actual}}|)) * 100$ = MAPE\r\n",
        "\r\n",
        "This method allows me to ignore instances in the test set where label data is missing; I only calculate the absolute percent error for labels that have data present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koLEd7uI_sor",
        "outputId": "d09b9cbf-4616-47a1-9a41-fea105b8eb86"
      },
      "source": [
        "test_set.dropna(inplace=True)\r\n",
        "whole_set_mean_absolute_percent_error = 0\r\n",
        "\r\n",
        "for index, row in test_set.iterrows():\r\n",
        "  absolute_percent_error_single_example = 0\r\n",
        "  test_example_np = row.iloc[:10].to_numpy().reshape(1,10)\r\n",
        "  prediction = model.predict(test_example_np)\r\n",
        "  num_non_zero_labels = 0\r\n",
        "  for label in range(0,len(prediction[0])):\r\n",
        "    if row.iloc[10+label] == 0:\r\n",
        "      continue\r\n",
        "    else:\r\n",
        "      num_non_zero_labels += 1\r\n",
        "      absolute_percent_error_single_example += abs((prediction[0][label] - row.iloc[10+label])/row.iloc[10+label] * 100)\r\n",
        "  whole_set_mean_absolute_percent_error += absolute_percent_error_single_example/num_non_zero_labels\r\n",
        "\r\n",
        "whole_set_mean_absolute_percent_error/=test_set.shape[0]\r\n",
        "print(\"Test Set Mean Absolute Percent Error: \" + str(whole_set_mean_absolute_percent_error) + \"%\")\r\n",
        "     \r\n",
        "\r\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set Mean Absolute Percent Error: 13.529602604145811%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLUi4wCVFaoe"
      },
      "source": [
        "# Further Considerations and Improvements\r\n",
        "\r\n",
        "\r\n",
        "*   Use unsupervised methods for data filling process\r\n",
        "\r\n",
        "> The model performs substantially worse in predicting labels where there was a large amount of missing data. One way to improve this might be using clustering or some variation of KNN to group similar records and fill the data based on their similarities.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Using a tool like Tensorboard to visualize the process in more detail\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Refactoring the code so that it is more modular/reusable\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Modeling the data \"in reverse\"\r\n",
        "\r\n",
        "\r\n",
        "> Using the measures of physical exertion to devise a scimmage that would produce those results (i.e. if we want the average player to finish at 85% HRmax, allow x players on a pitch size of y, with z amount of active rest).\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}